{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af55cd98-3652-4c65-84ad-72c153de6eb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Projet 8 : Déployez un modèle dans le cloud\n",
    "## Notebook preprocessing et featuring\n",
    "\n",
    "*Julie Neury-Ormanni*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185d48b-f6ec-4338-a9b2-82e1ba8bf1fc",
   "metadata": {},
   "source": [
    "### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "505b049f-a5d4-413c-b2df-8bc409052945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:41:54.976859Z",
     "iopub.status.busy": "2022-05-03T10:41:54.976500Z",
     "iopub.status.idle": "2022-05-03T10:41:55.055729Z",
     "shell.execute_reply": "2022-05-03T10:41:55.052758Z",
     "shell.execute_reply.started": "2022-05-03T10:41:54.976820Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c96208c37148aeb563c1a9eb4f5809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from pyspark.sql.functions import col, pandas_udf, element_at, PandasUDFType, split, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import gc\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, FloatType, StringType\n",
    "from pyspark.ml.feature import PCA, VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ffba3-d8c4-469a-8c74-b13913fd1b42",
   "metadata": {},
   "source": [
    "### Chargement des images depuis S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a816d33c-f257-499c-91c2-653309054b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T09:59:36.169158Z",
     "iopub.status.busy": "2022-05-03T09:59:36.168945Z",
     "iopub.status.idle": "2022-05-03T09:59:57.795666Z",
     "shell.execute_reply": "2022-05-03T09:59:57.794946Z",
     "shell.execute_reply.started": "2022-05-03T09:59:36.169135Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe62598212424d10a6e435e10609b556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|s3://p8neurybucke...|2022-04-29 12:48:08|  5278|[FF D8 FF E0 00 1...|\n",
      "|s3://p8neurybucke...|2022-04-29 13:27:25|  5277|[FF D8 FF E0 00 1...|\n",
      "|s3://p8neurybucke...|2022-04-29 13:27:34|  5277|[FF D8 FF E0 00 1...|\n",
      "|s3://p8neurybucke...|2022-04-29 12:48:08|  5272|[FF D8 FF E0 00 1...|\n",
      "|s3://p8neurybucke...|2022-04-29 13:27:36|  5264|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "path_image = 's3://p8neurybucket/Training-fruit/**'\n",
    "\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(path_image).limit(100)\n",
    "\n",
    "images.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c59e4a-6613-49f6-89b4-1eae280e48fc",
   "metadata": {},
   "source": [
    "### Preprocessing des images et transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d62edbe-64df-4b37-8e56-5c995bdb406e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:02:03.193583Z",
     "iopub.status.busy": "2022-05-03T10:02:03.193349Z",
     "iopub.status.idle": "2022-05-03T10:02:05.492773Z",
     "shell.execute_reply": "2022-05-03T10:02:05.492091Z",
     "shell.execute_reply.started": "2022-05-03T10:02:03.193558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a473ef8798e4ce6bfaf49f396dee289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "  # For some layers, output features will be multi-dimensional tensors.\n",
    "  # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    #ouput = np.array(output)\n",
    "    return pd.Series(output)\n",
    "\n",
    "model = ResNet50(weights=None, include_top=False)\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = ResNet50(weights=None, include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ab404-7c19-4e92-93a9-c9a296ee7b70",
   "metadata": {},
   "source": [
    "### Extraction des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7eb57b-9687-4dab-8c90-8170aabb28de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:04:03.026320Z",
     "iopub.status.busy": "2022-05-03T10:04:03.026100Z",
     "iopub.status.idle": "2022-05-03T10:04:03.801457Z",
     "shell.execute_reply": "2022-05-03T10:04:03.800819Z",
     "shell.execute_reply.started": "2022-05-03T10:04:03.026297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54316058f45f455ab81427ee81d44b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "@pandas_udf('array<float>')\n",
    "def featurize_udf(content_series_iter: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "    is a pandas Series of image data.\n",
    "    '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fbe9da-832f-4724-bfb8-f17280b4a1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:04:25.007686Z",
     "iopub.status.busy": "2022-05-03T10:04:25.007359Z",
     "iopub.status.idle": "2022-05-03T10:04:25.307069Z",
     "shell.execute_reply": "2022-05-03T10:04:25.306318Z",
     "shell.execute_reply.started": "2022-05-03T10:04:25.007647Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c634dd7f0242799d8d012f805ab551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_feat = images.repartition(16).select(col(\"path\"), featurize_udf(\"content\").alias(\"feats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f77ae80-330f-409f-80f1-dd3e0b5b7ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:04:36.164241Z",
     "iopub.status.busy": "2022-05-03T10:04:36.164010Z",
     "iopub.status.idle": "2022-05-03T10:05:07.585560Z",
     "shell.execute_reply": "2022-05-03T10:05:07.584970Z",
     "shell.execute_reply.started": "2022-05-03T10:04:36.164217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17822e7c95a2455fb37b0949296ed623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|                path|               feats|          label|\n",
      "+--------------------+--------------------+---------------+\n",
      "|s3://p8neurybucke...|[0.0, 5.6815786, ...| Apple Braeburn|\n",
      "|s3://p8neurybucke...|[0.0, 5.8709025, ...| Apple Braeburn|\n",
      "|s3://p8neurybucke...|[0.0, 5.6673794, ...|Apple Pink Lady|\n",
      "+--------------------+--------------------+---------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# Getting the label by splitting the path of the image and getting its last directory\n",
    "df_feat = (df_feat.withColumn('label',element_at(split(df_feat['path'],\"/\"),-2)))\n",
    "\n",
    "df_feat.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef643c5-609d-48f1-9e89-296db25c995f",
   "metadata": {},
   "source": [
    "### Réduction des features par ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e742839-6534-4ae9-89da-62f373f048e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:05:39.806598Z",
     "iopub.status.busy": "2022-05-03T10:05:39.806377Z",
     "iopub.status.idle": "2022-05-03T10:05:39.865686Z",
     "shell.execute_reply": "2022-05-03T10:05:39.865142Z",
     "shell.execute_reply.started": "2022-05-03T10:05:39.806574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1991b918609446479bfb4ee07e047dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pca_transformation(df, n_components=50, col_image='feats'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applique un algorithme de PCA sur l'ensemble des images pour réduire la dimension de chaque image \n",
    "    du jeu de données.\n",
    "    \n",
    "    Paramètres:\n",
    "    df(pyspark dataFrame): contient une colonne avec les données images\n",
    "    n_components(int): nombre de dimensions à conserver\n",
    "    col_image(string): nom de la colonne où récupérer les données images\n",
    "    \"\"\"\n",
    "\n",
    "    # Initilisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Les données images sont converties au format vecteur dense\n",
    "    ud_f = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    df = df.withColumn('feats', ud_f('feats'))\n",
    "    \n",
    "    standardizer = StandardScaler(inputCol='feats', outputCol=\"scaledFeatures\",\n",
    "                                  withStd=True, withMean=True)\n",
    "    model_std = standardizer.fit(df)\n",
    "    df = model_std.transform(df)\n",
    "\n",
    "    # Entrainement de l'algorithme\n",
    "    pca = PCA(k=n_components, inputCol='scaledFeatures', outputCol='pcaFeatures')\n",
    "    model_pca = pca.fit(df)\n",
    "\n",
    "    # Transformation des images sur les k premières composantes\n",
    "    df = model_pca.transform(df)\n",
    "\n",
    "    df = df.filter(df.pcaFeatures.isNotNull())\n",
    "    \n",
    "    # Affiche le temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} minutes\".format((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185835f1-25eb-4d30-b714-4a498059b917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:05:42.004733Z",
     "iopub.status.busy": "2022-05-03T10:05:42.004507Z",
     "iopub.status.idle": "2022-05-03T10:35:09.965784Z",
     "shell.execute_reply": "2022-05-03T10:35:09.965162Z",
     "shell.execute_reply.started": "2022-05-03T10:05:42.004708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f969842c9cb6420ab35a1a245b1bc9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---R?duction dimmensionnelle---\n",
      "Temps d'execution 28.88 minutes\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+\n",
      "|                path|               feats|         label|      scaledFeatures|         pcaFeatures|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+\n",
      "|s3://p8neurybucke...|[0.0,5.4075827598...|Apple Braeburn|[0.0,-0.477697638...|[0.13785683017128...|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "print(\"---Réduction dimmensionnelle---\")\n",
    "pca_df = pca_transformation(df_feat)\n",
    "pca_df.show(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703a9dc-9312-44d1-b81f-c767f57cd66e",
   "metadata": {},
   "source": [
    "### Export des features en parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe073e43-5adb-4c89-a6ce-5e73c38f854b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:37:58.838760Z",
     "iopub.status.busy": "2022-05-03T10:37:58.838534Z",
     "iopub.status.idle": "2022-05-03T10:39:26.406145Z",
     "shell.execute_reply": "2022-05-03T10:39:26.405490Z",
     "shell.execute_reply.started": "2022-05-03T10:37:58.838736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b44a6407b148c58bb7f366fa82b0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca_df.select(['path', 'label', 'pcaFeatures']).write.parquet('s3://p8neurybucket/output_feat.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a24eb1-a57e-4e89-9cb3-d8ddff500abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:40:31.791254Z",
     "iopub.status.busy": "2022-05-03T10:40:31.791019Z",
     "iopub.status.idle": "2022-05-03T10:40:41.092733Z",
     "shell.execute_reply": "2022-05-03T10:40:41.092005Z",
     "shell.execute_reply.started": "2022-05-03T10:40:31.791230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb4883e948f4603834fc716487be1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+\n",
      "|                path|          label|         pcaFeatures|\n",
      "+--------------------+---------------+--------------------+\n",
      "|s3://p8neurybucke...| Apple Braeburn|[0.09293418329549...|\n",
      "|s3://p8neurybucke...| Apple Braeburn|[-135952.93232378...|\n",
      "|s3://p8neurybucke...| Apple Braeburn|[0.01688021266067...|\n",
      "|s3://p8neurybucke...|Apple Pink Lady|[0.12184944035153...|\n",
      "|s3://p8neurybucke...| Apple Braeburn|[-0.0663827312183...|\n",
      "+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet('s3://p8neurybucket/output_feat.parquet')\n",
    "parquetFile.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a935a0-2a26-4f5e-972d-074c59b26985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:42:01.794146Z",
     "iopub.status.busy": "2022-05-03T10:42:01.793897Z",
     "iopub.status.idle": "2022-05-03T10:42:15.099008Z",
     "shell.execute_reply": "2022-05-03T10:42:15.098413Z",
     "shell.execute_reply.started": "2022-05-03T10:42:01.794120Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e004eb0ea34410bcdbccf411092512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+\n",
      "|                path|          label|         pcaFeatures|   pcaFeaturesString|\n",
      "+--------------------+---------------+--------------------+--------------------+\n",
      "|s3://p8neurybucke...| Apple Braeburn|[0.09293418329549...|0.092934183295497...|\n",
      "|s3://p8neurybucke...| Apple Braeburn|[-135952.93232378...|-135952.932323783...|\n",
      "|s3://p8neurybucke...| Apple Braeburn|[0.01688021266067...|0.016880212660676...|\n",
      "|s3://p8neurybucke...|Apple Pink Lady|[0.12184944035153...|0.121849440351535...|\n",
      "|s3://p8neurybucke...| Apple Braeburn|[-0.0663827312183...|-0.06638273121833...|\n",
      "+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "sparse_format_udf = udf(lambda x: ','.join([str(elem) for elem in x]), StringType())\n",
    "\n",
    "parquetFile = parquetFile.withColumn('pcaFeaturesString', sparse_format_udf(parquetFile.pcaFeatures))\n",
    "\n",
    "parquetFile.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c672560b-159b-430f-8dca-ed7f0b9cfe64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T10:42:20.507429Z",
     "iopub.status.busy": "2022-05-03T10:42:20.507199Z",
     "iopub.status.idle": "2022-05-03T10:42:20.783195Z",
     "shell.execute_reply": "2022-05-03T10:42:20.782326Z",
     "shell.execute_reply.started": "2022-05-03T10:42:20.507404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b13c3348bb6494d9b294d27e74b39f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o267.csv.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: All access to this object has been disabled (Service: Amazon S3; Status Code: 403; Error Code: AllAccessDisabled; Request ID: EYGQPJTP8AE0W45Y; S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=; Proxy: null), S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:421)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:247)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:210)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1690)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.exists(EmrFileSystem.java:436)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:124)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: All access to this object has been disabled (Service: Amazon S3; Status Code: 403; Error Code: AllAccessDisabled; Request ID: EYGQPJTP8AE0W45Y; S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=; Proxy: null), S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1862)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1415)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5386)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:108)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:135)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:412)\n",
      "\t... 44 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1372, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o267.csv.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: All access to this object has been disabled (Service: Amazon S3; Status Code: 403; Error Code: AllAccessDisabled; Request ID: EYGQPJTP8AE0W45Y; S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=; Proxy: null), S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:421)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:247)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:210)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1690)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.exists(EmrFileSystem.java:436)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:124)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: All access to this object has been disabled (Service: Amazon S3; Status Code: 403; Error Code: AllAccessDisabled; Request ID: EYGQPJTP8AE0W45Y; S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=; Proxy: null), S3 Extended Request ID: a85D9DfcPn+NZSTSqqAlnLElriu1iTwuz+1ypGZbLeyPdfZ7Q8L54SEHthequrGLagdd4fcg97w=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1862)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1415)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5445)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5392)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5386)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:108)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:135)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:412)\n",
      "\t... 44 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.select(['path', 'label', 'pcaFeaturesString']) \\\n",
    "    .write.option(\"delimiter\", \"\\t\").option(\"header\", True) \\\n",
    "    .csv('s3://p8-s3-cindygs/output_feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a2155-a303-4bbf-aefb-11db701cbffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
